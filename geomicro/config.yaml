jobs: 1000
local-cores: 1
cluster: "mkdir -p logs/slurm && sbatch --parsable --time={resources.time_min} --mem={resources.mem_mb} -c {resources.cpus} -o logs/slurm/{rule}_{wildcards} -e logs/slurm/{rule}_{wildcards}"
cluster-cancel: "scancel"
use-conda: true
conda-frontend: mamba
use-singularity: true
default-resources: [cpus=1, mem_mb=8000, time_min=120, heavy_network=0, gpu=0, disk_mb=100]
#resources: [disk_mb=2000000]
resources: [cpus=224, mem_mb=100000000, heavy_network=1, disk_mb=2000000]
latency-wait: 60
cluster-status: "slurm-status.py"
max-status-checks-per-second: 1
singularity-args: "--home /home/$USER:/home/$USER --bind /nfs/turbo/lsa-gdick2/"