jobs: 1000
local-cores: 1
cluster: "sbatch --parsable --account={resources.account} --partition={resources.partition} --time={resources.time_min} --mem={resources.mem_mb} -c {resources.cpus} --gres=gpu:{resources.gpu} -o logs/slurm/{rule}_{wildcards} -e logs/slurm/{rule}_{wildcards}"
cluster-cancel: "scancel"
use-conda: true
conda-frontend: mamba
use-singularity: true
default-resources: [cpus=1, mem_mb=8000, time_min=120, account=gdick2, partition=standard, heavy_network=0, gpu=0, disk_mb=100]
#resources: [disk_mb=2000000]
#resources: [cpus=20000, mem_mb=100000000, heavy_network=1, disk_mb=2000000]
latency-wait: 60
cluster-status: "slurm-status.py"
max-status-checks-per-second: 1
singularity-args: "--home /home/$USER:/home/$USER --bind /scratch/gdick_root/ --bind /nfs/turbo/lsa-gdick2/ --bind /gpfs/accounts/gdick_root/"